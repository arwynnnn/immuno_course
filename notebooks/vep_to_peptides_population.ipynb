{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From VEP variants → transcripts → proteins → WT peptide contexts → peptides for NetMHCpan / NetMHCIIpan\n",
        "\n",
        "This notebook:\n",
        "\n",
        "- Reads a VEP tab-delimited output file produced with a command like:\n",
        "\n",
        "  ```bash\n",
        "  ./vep --af --appris --biotype --buffer_size 500 --check_existing --distance 5000        --mane --polyphen b --pubmed --regulatory --show_ref_allele --sift b        --species homo_sapiens --symbol --transcript_version --tsl --uploaded_allele        --cache --input_file input_data --output_file output_file\n",
        "  ```\n",
        "\n",
        "- Keeps **protein-altering** variants (missense, stop gained, frameshift, inframe indels, etc.)\n",
        "- Picks **one transcript per (variant, gene)** using MANE, APPRIS, TSL, and IMPACT\n",
        "- Uses **Ensembl REST** to map transcript IDs (ENST) to protein sequences (ENSP)\n",
        "- Extracts **WT and MUT peptide contexts** around the mutated residue:\n",
        "    - ±15 aa for MHC-II (31-mers)\n",
        "    - ±8 aa for MHC-I (17-mers)\n",
        "- Generates **overlapping WT/MUT peptides**:\n",
        "    - Class I: 8–11-mers\n",
        "    - Class II: 13–15-mers\n",
        "  and keeps only peptides that contain the mutated residue.\n",
        "- Adds IDs for downstream pairing:\n",
        "    - `peptide_id`: unique row ID\n",
        "    - `pair_id`: key that matches WT/MUT windows (same variant, class, length, mut_offset)\n",
        "    - `mut_offset`: position of the mutated residue within the peptide (0-based)\n",
        "- Writes:\n",
        "    - `vep_peptide_contexts.tsv` – variant → transcript → protein → context\n",
        "    - `peptides_for_netmhc.tsv` – all WT/MUT peptides and metadata\n",
        "    - `netmhcpan_classI_peptides.txt` – list of class I peptides (for NetMHCpan)\n",
        "    - `netmhciipan_classII_peptides.txt` – list of class II peptides (for NetMHCIIpan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed, install dependencies (uncomment & run once)\n",
        "# !pip install pandas requests tqdm\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -----------------------\n",
        "# User configuration\n",
        "# -----------------------\n",
        "\n",
        "# Path to your VEP tab-separated output file\n",
        "# Change this to the path of the VEP file you already have\n",
        "VEP_FILE = \"C:\\Users\\Kacper\\Desktop\\immuno\\data\\qZWuZht1PnMvtm20.txt\"  # <-- EDIT THIS\n",
        "\n",
        "# Output file with peptide contexts\n",
        "OUTPUT_TSV = \"vep_peptide_contexts.tsv\"\n",
        "\n",
        "# Ensembl REST server:\n",
        "# - For GRCh38: https://rest.ensembl.org\n",
        "# - For GRCh37: https://grch37.rest.ensembl.org\n",
        "ENSEMBL_REST_SERVER = \"https://rest.ensembl.org\"  # change if you used GRCh37\n",
        "\n",
        "# How many amino acids to keep on each side of the mutated residue\n",
        "FLANK_MHCII = 15  # → 31-mers\n",
        "FLANK_MHCI = 8    # → 17-mers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_vep_table(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Read a VEP tab-delimited output file.\n",
        "\n",
        "    Assumes the header line starts with '#Uploaded_variation' and skips\n",
        "    earlier comment lines starting with '##'.\n",
        "    \"\"\"\n",
        "    header_line = None\n",
        "    header_idx = None\n",
        "    with open(path, \"r\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if line.startswith(\"#Uploaded_variation\"):\n",
        "                header_line = line.strip().lstrip(\"#\")\n",
        "                header_idx = i\n",
        "                break\n",
        "\n",
        "    if header_line is None:\n",
        "        raise ValueError(\"Could not find header line starting with '#Uploaded_variation' in VEP file\")\n",
        "\n",
        "    colnames = header_line.split(\"\\t\")\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=colnames,\n",
        "        skiprows=header_idx + 1,\n",
        "        dtype=str  # keep everything as string\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "vep_df = read_vep_table(VEP_FILE)\n",
        "print(\"Loaded VEP file with shape:\", vep_df.shape)\n",
        "print(\"First few columns:\", list(vep_df.columns)[:15])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Restrict to protein-altering variants\n",
        "# -----------------------------\n",
        "\n",
        "# These are the consequence terms we consider as protein-altering.\n",
        "# You can adjust this list if needed.\n",
        "protein_altering_terms = [\n",
        "    \"missense_variant\",\n",
        "    \"stop_gained\",\n",
        "    \"stop_lost\",\n",
        "    \"start_lost\",\n",
        "    \"frameshift_variant\",\n",
        "    \"inframe_insertion\",\n",
        "    \"inframe_deletion\",\n",
        "    \"protein_altering_variant\",\n",
        "]\n",
        "\n",
        "\n",
        "def is_protein_altering(cons: str) -> bool:\n",
        "    if pd.isna(cons):\n",
        "        return False\n",
        "    s = str(cons)\n",
        "    return any(term in s for term in protein_altering_terms)\n",
        "\n",
        "\n",
        "# Keep only protein-coding transcripts with protein-altering consequences\n",
        "if \"BIOTYPE\" not in vep_df.columns:\n",
        "    raise ValueError(\"VEP file is missing BIOTYPE column. Make sure you used --biotype in VEP.\")\n",
        "\n",
        "pc_mask = vep_df[\"BIOTYPE\"] == \"protein_coding\"\n",
        "altering_mask = vep_df[\"Consequence\"].apply(is_protein_altering)\n",
        "\n",
        "# Require that Protein_position and Amino_acids are filled\n",
        "pos_mask = vep_df[\"Protein_position\"].notna() & (vep_df[\"Protein_position\"] != \"-\")\n",
        "aa_mask = vep_df[\"Amino_acids\"].notna() & (vep_df[\"Amino_acids\"] != \"-\")\n",
        "\n",
        "vep_pc = vep_df[pc_mask & altering_mask & pos_mask & aa_mask].copy()\n",
        "print(\"Protein-coding, protein-altering rows with AA info:\", vep_pc.shape)\n",
        "\n",
        "\n",
        "def parse_aa_change(aa_str: str):\n",
        "    if pd.isna(aa_str):\n",
        "        return (None, None)\n",
        "    s = str(aa_str)\n",
        "    if \"/\" in s:\n",
        "        wt, alt = s.split(\"/\", 1)\n",
        "        wt = wt or None\n",
        "        alt = alt or None\n",
        "        return wt, alt\n",
        "    return (None, None)\n",
        "\n",
        "\n",
        "def parse_protein_pos(pos_str: str):\n",
        "    if pd.isna(pos_str):\n",
        "        return None\n",
        "    s = str(pos_str)\n",
        "    if \"-\" in s:\n",
        "        s = s.split(\"-\", 1)[0]  # if it's a range, take the first position\n",
        "    s = s.strip()\n",
        "    if not s:\n",
        "        return None\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "\n",
        "vep_pc[\"aa_wt\"], vep_pc[\"aa_alt\"] = zip(*vep_pc[\"Amino_acids\"].map(parse_aa_change))\n",
        "vep_pc[\"prot_pos\"] = vep_pc[\"Protein_position\"].map(parse_protein_pos)\n",
        "\n",
        "# Keep only rows with a clear protein position and WT amino acid\n",
        "vep_pc = vep_pc[vep_pc[\"prot_pos\"].notna() & vep_pc[\"aa_wt\"].notna()].copy()\n",
        "vep_pc[\"prot_pos\"] = vep_pc[\"prot_pos\"].astype(int)\n",
        "\n",
        "print(\"After parsing AA and positions:\", vep_pc.shape)\n",
        "\n",
        "if vep_pc.empty:\n",
        "    print(\"⚠️ No protein-altering variants with AA info found in this file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# Choose one transcript per (variant, gene)\n",
        "# --------------------------------------\n",
        "\n",
        "def _is_defined(x) -> bool:\n",
        "    return pd.notna(x) and str(x) not in {\"\", \"-\"}\n",
        "\n",
        "\n",
        "def _parse_tsl(x):\n",
        "    if not _is_defined(x):\n",
        "        return 99\n",
        "    s = str(x)\n",
        "    # VEP TSL values can look like '1', 'TSL1', etc.\n",
        "    s = s.replace(\"TSL\", \"\")\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return 99\n",
        "\n",
        "\n",
        "def _appris_rank(x):\n",
        "    # Lower is better\n",
        "    mapping = {\n",
        "        \"P1\": 0,\n",
        "        \"P2\": 1,\n",
        "        \"P3\": 2,\n",
        "        \"P4\": 3,\n",
        "        \"P5\": 4,\n",
        "        \"P6\": 5,\n",
        "        \"A1\": 6,  # alternate isoform\n",
        "    }\n",
        "    if not _is_defined(x):\n",
        "        return 99\n",
        "    return mapping.get(str(x), 99)\n",
        "\n",
        "\n",
        "def _impact_rank(x):\n",
        "    mapping = {\n",
        "        \"HIGH\": 0,\n",
        "        \"MODERATE\": 1,\n",
        "        \"LOW\": 2,\n",
        "        \"MODIFIER\": 3,\n",
        "    }\n",
        "    if not _is_defined(x):\n",
        "        return 99\n",
        "    return mapping.get(str(x), 99)\n",
        "\n",
        "\n",
        "def choose_transcript(group: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Given all VEP rows for one (variant, gene), pick one transcript.\n",
        "\n",
        "    Priority:\n",
        "    - MANE_PLUS_CLINICAL (if present)\n",
        "    - MANE_SELECT\n",
        "    - Higher IMPACT (HIGH > MODERATE > LOW > MODIFIER)\n",
        "    - Better APPRIS (P1 > P2 > ...)\n",
        "    - Better TSL (1 > 2 > ...)\n",
        "    \"\"\"\n",
        "    g = group.copy()\n",
        "\n",
        "    if \"MANE_PLUS_CLINICAL\" in g.columns:\n",
        "        g[\"_has_mane_plus\"] = g[\"MANE_PLUS_CLINICAL\"].map(_is_defined)\n",
        "    else:\n",
        "        g[\"_has_mane_plus\"] = False\n",
        "\n",
        "    if \"MANE_SELECT\" in g.columns:\n",
        "        g[\"_has_mane_select\"] = g[\"MANE_SELECT\"].map(_is_defined)\n",
        "    else:\n",
        "        g[\"_has_mane_select\"] = False\n",
        "\n",
        "    g[\"_impact_rank\"] = g[\"IMPACT\"].map(_impact_rank) if \"IMPACT\" in g.columns else 99\n",
        "    g[\"_appris_rank\"] = g[\"APPRIS\"].map(_appris_rank) if \"APPRIS\" in g.columns else 99\n",
        "    g[\"_tsl_rank\"] = g[\"TSL\"].map(_parse_tsl) if \"TSL\" in g.columns else 99\n",
        "\n",
        "    g = g.sort_values(\n",
        "        by=[\"_has_mane_plus\", \"_has_mane_select\", \"_impact_rank\", \"_appris_rank\", \"_tsl_rank\"],\n",
        "        ascending=[False, False, True, True, True],\n",
        "        kind=\"mergesort\",  # stable sort\n",
        "    )\n",
        "\n",
        "    chosen = g.iloc[0].copy()\n",
        "    for tmp in [\"_has_mane_plus\", \"_has_mane_select\", \"_impact_rank\", \"_appris_rank\", \"_tsl_rank\"]:\n",
        "        if tmp in chosen.index:\n",
        "            chosen = chosen.drop(tmp)\n",
        "    return chosen\n",
        "\n",
        "\n",
        "picked_rows = []\n",
        "if vep_pc.empty:\n",
        "    print(\"No protein-altering rows; picked_df will be empty.\")\n",
        "    picked_df = vep_pc.copy()\n",
        "else:\n",
        "    group_cols = [\"Uploaded_variation\", \"SYMBOL\"]\n",
        "    for _, group in tqdm(vep_pc.groupby(group_cols), desc=\"Choosing transcripts\"):\n",
        "        picked_rows.append(choose_transcript(group))\n",
        "    picked_df = pd.DataFrame(picked_rows).reset_index(drop=True)\n",
        "\n",
        "print(\"After picking one transcript per (variant, gene):\", picked_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# Map transcripts (ENST) to translations (ENSP) via Ensembl REST\n",
        "# --------------------------------------\n",
        "\n",
        "if \"Feature\" not in picked_df.columns:\n",
        "    raise ValueError(\"VEP file is missing 'Feature' column (transcript ID).\")\n",
        "\n",
        "picked_df[\"transcript_id\"] = picked_df[\"Feature\"].astype(str)\n",
        "\n",
        "unique_transcripts = sorted(picked_df[\"transcript_id\"].dropna().unique())\n",
        "print(\"Unique transcripts to map:\", len(unique_transcripts))\n",
        "\n",
        "\n",
        "def fetch_transcript_to_translation(transcript_ids, server: str = ENSEMBL_REST_SERVER, sleep_between: float = 0.1):\n",
        "    \"\"\"\n",
        "    For each transcript ID (ENST...), call Ensembl REST /lookup/id to find the translation ID (ENSP...).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict: {transcript_id -> translation_id}\n",
        "    \"\"\"\n",
        "    if not transcript_ids:\n",
        "        return {}\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n",
        "    mapping = {}\n",
        "\n",
        "    for tid in tqdm(transcript_ids, desc=\"Mapping transcripts to translations\"):\n",
        "        if not tid or tid in mapping:\n",
        "            continue\n",
        "\n",
        "        # Try with version first, then without version if needed\n",
        "        candidates = [tid]\n",
        "        if \".\" in tid:\n",
        "            candidates.append(tid.split(\".\", 1)[0])\n",
        "\n",
        "        translation_id = None\n",
        "        for cid in candidates:\n",
        "            url = server.rstrip(\"/\") + f\"/lookup/id/{cid}?expand=1\"\n",
        "            r = requests.get(url, headers=headers)\n",
        "            if not r.ok:\n",
        "                continue\n",
        "            data = r.json()\n",
        "            trans_info = data.get(\"Translation\") or data.get(\"translation\")\n",
        "            if trans_info and \"id\" in trans_info:\n",
        "                translation_id = trans_info[\"id\"]\n",
        "                break\n",
        "\n",
        "        if translation_id is not None:\n",
        "            mapping[tid] = translation_id\n",
        "\n",
        "        time.sleep(sleep_between)\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "transcript_to_translation = fetch_transcript_to_translation(unique_transcripts, server=ENSEMBL_REST_SERVER)\n",
        "print(\"Mapped transcripts with translations:\", len(transcript_to_translation))\n",
        "\n",
        "picked_df[\"translation_id\"] = picked_df[\"transcript_id\"].map(transcript_to_translation)\n",
        "print(\"Rows without translation:\", picked_df[\"translation_id\"].isna().sum())\n",
        "\n",
        "# Drop rows without translation ID\n",
        "picked_df = picked_df[picked_df[\"translation_id\"].notna()].copy()\n",
        "print(\"Rows after dropping those without translation:\", picked_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# Fetch protein sequences for translation IDs (ENSP) via Ensembl REST\n",
        "# --------------------------------------\n",
        "\n",
        "unique_proteins = sorted(picked_df[\"translation_id\"].dropna().unique())\n",
        "print(\"Unique protein IDs to fetch:\", len(unique_proteins))\n",
        "\n",
        "\n",
        "def fetch_protein_seqs_ensembl(\n",
        "    protein_ids,\n",
        "    server: str = ENSEMBL_REST_SERVER,\n",
        "    chunk_size: int = 50,\n",
        "    sleep_between: float = 0.1,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Fetch protein sequences from Ensembl REST /sequence/id endpoint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    protein_ids : list of str\n",
        "        Ensembl protein IDs (ENSP...), without or with version.\n",
        "    server : str\n",
        "        REST server base URL.\n",
        "    chunk_size : int\n",
        "        How many IDs to request per POST.\n",
        "    sleep_between : float\n",
        "        Seconds to sleep between chunks (be nice to the server).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict: {protein_id -> amino acid sequence}\n",
        "    \"\"\"\n",
        "    protein_ids = [pid for pid in protein_ids if pid]\n",
        "    if not protein_ids:\n",
        "        return {}\n",
        "\n",
        "    unique_ids = list(dict.fromkeys(protein_ids))  # preserve order + unique\n",
        "\n",
        "    url = server.rstrip(\"/\") + \"/sequence/id\"\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n",
        "\n",
        "    seqs = {}\n",
        "    for i in tqdm(range(0, len(unique_ids), chunk_size), desc=\"Fetching protein sequences\"):\n",
        "        chunk = unique_ids[i : i + chunk_size]\n",
        "        payload = json.dumps({\"ids\": chunk})\n",
        "        r = requests.post(url, headers=headers, data=payload)\n",
        "        if not r.ok:\n",
        "            print(\"Error fetching sequences for IDs (showing first 5):\", chunk[:5])\n",
        "            r.raise_for_status()\n",
        "        data = r.json()\n",
        "        # data is a list of dicts with 'id' and 'seq' keys\n",
        "        for entry in data:\n",
        "            pid = entry.get(\"id\")\n",
        "            seq = entry.get(\"seq\")\n",
        "            if pid and seq:\n",
        "                seqs[pid] = seq\n",
        "        time.sleep(sleep_between)\n",
        "\n",
        "    return seqs\n",
        "\n",
        "\n",
        "protein_seqs = fetch_protein_seqs_ensembl(unique_proteins, server=ENSEMBL_REST_SERVER)\n",
        "print(\"Fetched sequences for\", len(protein_seqs), \"proteins\")\n",
        "\n",
        "picked_df[\"protein_seq\"] = picked_df[\"translation_id\"].map(protein_seqs)\n",
        "print(\"Rows with missing protein sequence:\", picked_df[\"protein_seq\"].isna().sum())\n",
        "\n",
        "# Drop rows without sequences\n",
        "picked_df = picked_df[picked_df[\"protein_seq\"].notna()].copy()\n",
        "print(\"Rows after dropping missing sequences:\", picked_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# Extract WT peptide contexts (±15, ±8)\n",
        "# --------------------------------------\n",
        "\n",
        "def extract_context(seq: str, pos_1based: int, flank: int) -> str:\n",
        "    \"\"\"\n",
        "    Extract a window of amino acids around pos_1based (1-based index).\n",
        "    Returns a (clipped) window of length <= 2*flank+1.\n",
        "    \"\"\"\n",
        "    if seq is None:\n",
        "        return None\n",
        "    try:\n",
        "        pos0 = int(pos_1based) - 1\n",
        "    except (TypeError, ValueError):\n",
        "        return None\n",
        "    if pos0 < 0 or pos0 >= len(seq):\n",
        "        return None\n",
        "    start = max(0, pos0 - flank)\n",
        "    end = min(len(seq), pos0 + flank + 1)  # end is exclusive\n",
        "    return seq[start:end]\n",
        "\n",
        "\n",
        "picked_df[\"mhcII_wt_context\"] = picked_df.apply(\n",
        "    lambda r: extract_context(r[\"protein_seq\"], r[\"prot_pos\"], FLANK_MHCII),\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "picked_df[\"mhcI_wt_context\"] = picked_df.apply(\n",
        "    lambda r: extract_context(r[\"protein_seq\"], r[\"prot_pos\"], FLANK_MHCI),\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "print(\n",
        "    picked_df[\n",
        "        [\n",
        "            \"Uploaded_variation\",\n",
        "            \"SYMBOL\",\n",
        "            \"Feature\",\n",
        "            \"translation_id\",\n",
        "            \"aa_wt\",\n",
        "            \"aa_alt\",\n",
        "            \"prot_pos\",\n",
        "            \"mhcII_wt_context\",\n",
        "            \"mhcI_wt_context\",\n",
        "        ]\n",
        "    ].head()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# Optional: mutant peptide contexts (simple missense)\n",
        "# --------------------------------------\n",
        "\n",
        "def extract_mutant_context(row, flank: int):\n",
        "    seq = row[\"protein_seq\"]\n",
        "    pos = row[\"prot_pos\"]\n",
        "    aa_alt = row[\"aa_alt\"]\n",
        "    aa_wt = row[\"aa_wt\"]\n",
        "\n",
        "    # Only handle simple single-AA substitutions\n",
        "    if seq is None or pd.isna(pos) or aa_alt is None or aa_wt is None:\n",
        "        return None\n",
        "    if len(str(aa_alt)) != 1 or len(str(aa_wt)) != 1:\n",
        "        return None\n",
        "\n",
        "    pos0 = int(pos) - 1\n",
        "    if pos0 < 0 or pos0 >= len(seq):\n",
        "        return None\n",
        "\n",
        "    seq_mut = seq[:pos0] + str(aa_alt) + seq[pos0 + 1 :]\n",
        "    return extract_context(seq_mut, pos, flank)\n",
        "\n",
        "\n",
        "picked_df[\"mhcII_mut_context\"] = picked_df.apply(\n",
        "    lambda r: extract_mutant_context(r, FLANK_MHCII),\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "picked_df[\"mhcI_mut_context\"] = picked_df.apply(\n",
        "    lambda r: extract_mutant_context(r, FLANK_MHCI),\n",
        "    axis=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# Save variant → transcript → protein → context table\n",
        "# --------------------------------------\n",
        "\n",
        "# Core columns\n",
        "output_cols = [\n",
        "    \"Uploaded_variation\",\n",
        "    \"Location\",\n",
        "    \"Allele\",\n",
        "    \"Consequence\",\n",
        "    \"IMPACT\",\n",
        "    \"SYMBOL\",\n",
        "    \"Gene\",\n",
        "    \"Feature\",        # transcript ID\n",
        "    \"transcript_id\",\n",
        "    \"translation_id\", # Ensembl protein ID\n",
        "    \"aa_wt\",\n",
        "    \"aa_alt\",\n",
        "    \"prot_pos\",\n",
        "    \"mhcII_wt_context\",\n",
        "    \"mhcI_wt_context\",\n",
        "    \"mhcII_mut_context\",\n",
        "    \"mhcI_mut_context\",\n",
        "]\n",
        "\n",
        "# Also keep MANE / APPRIS / TSL if available\n",
        "for extra in [\"MANE_SELECT\", \"MANE_PLUS_CLINICAL\", \"MANE\", \"APPRIS\", \"TSL\"]:\n",
        "    if extra in picked_df.columns and extra not in output_cols:\n",
        "        output_cols.append(extra)\n",
        "\n",
        "# Ensure all requested columns exist (create empty if necessary)\n",
        "for col in output_cols:\n",
        "    if col not in picked_df.columns:\n",
        "        picked_df[col] = pd.NA\n",
        "\n",
        "output_df = picked_df[output_cols].copy()\n",
        "output_df.to_csv(OUTPUT_TSV, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Saved context table to: {OUTPUT_TSV}\")\n",
        "output_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate overlapping peptides for NetMHCpan / NetMHCIIpan\n",
        "\n",
        "We now:\n",
        "\n",
        "- Generate overlapping sliding windows from the contexts:\n",
        "  - Class I (NetMHCpan): 8, 9, 10, 11-mers\n",
        "  - Class II (NetMHCIIpan): 13, 14, 15-mers\n",
        "- Keep only peptides that contain the mutated residue\n",
        "- Add:\n",
        "  - `mut_offset`: 0-based index of mutated residue within the peptide\n",
        "  - `pair_id`: matches WT/MUT windows for the same variant/class/length/offset\n",
        "  - `peptide_id`: unique row ID\n",
        "- Save:\n",
        "  - `peptides_for_netmhc.tsv`\n",
        "  - `netmhcpan_classI_peptides.txt`\n",
        "  - `netmhciipan_classII_peptides.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# Helper: index of the mutated residue inside each context\n",
        "# --------------------------------------\n",
        "\n",
        "def mutated_index_in_context(row, flank: int, context_col: str):\n",
        "    \"\"\"\n",
        "    Given a row with:\n",
        "      - protein_seq (full protein)\n",
        "      - prot_pos (1-based position in protein)\n",
        "      - context_col (mhcI_*_context or mhcII_*_context)\n",
        "    and the flank used to build that context,\n",
        "    return the 0-based index of the mutated residue within that context.\n",
        "\n",
        "    If information is missing, returns None.\n",
        "    \"\"\"\n",
        "    seq = row.get(\"protein_seq\")\n",
        "    pos = row.get(\"prot_pos\")\n",
        "    context = row.get(context_col)\n",
        "\n",
        "    if seq is None or pd.isna(pos) or context is None or pd.isna(context):\n",
        "        return None\n",
        "\n",
        "    pos0 = int(pos) - 1  # 0-based in full protein\n",
        "    if pos0 < 0 or pos0 >= len(seq):\n",
        "        return None\n",
        "\n",
        "    start = max(0, pos0 - flank)\n",
        "    # context was built as: seq[start:end]\n",
        "    return pos0 - start  # 0-based index within this context\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# Helper: generate all overlapping windows that include the mutation\n",
        "# --------------------------------------\n",
        "\n",
        "def windows_with_mutation(context: str, mut_idx: int, length: int):\n",
        "    \"\"\"\n",
        "    Return list of (peptide, mut_offset) where:\n",
        "      - peptide is a substring of `context` of given `length`\n",
        "      - mut_offset is the 0-based index of the mutated residue within the peptide.\n",
        "\n",
        "    Only windows that contain position `mut_idx` are returned.\n",
        "    \"\"\"\n",
        "    if context is None or pd.isna(context):\n",
        "        return []\n",
        "    n = len(context)\n",
        "    if n < length:\n",
        "        return []\n",
        "\n",
        "    peptides = []\n",
        "    for start in range(0, n - length + 1):\n",
        "        end = start + length\n",
        "        if start <= mut_idx < end:\n",
        "            pep = context[start:end]\n",
        "            if len(pep) == length:\n",
        "                mut_offset = mut_idx - start\n",
        "                peptides.append((pep, mut_offset))\n",
        "    return peptides\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# Generate peptides for each row in picked_df\n",
        "# --------------------------------------\n",
        "\n",
        "def make_peptide_records_for_row(row):\n",
        "    \"\"\"\n",
        "    From one variant / transcript row, generate all WT and MUT peptides\n",
        "    for:\n",
        "      - Class I: lengths 8–11 aa\n",
        "      - Class II: lengths 13–15 aa\n",
        "\n",
        "    Only peptides that contain the mutated residue are kept.\n",
        "\n",
        "    For each peptide we store:\n",
        "      - mut_offset: 0-based index of the mutated residue within the peptide\n",
        "      - pair_id:   variant+class+length+mut_offset -> used to pair WT vs MUT\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    # Class I lengths (NetMHCpan)\n",
        "    classI_lengths = range(8, 12)   # 8, 9, 10, 11\n",
        "    # Class II lengths (NetMHCIIpan)\n",
        "    classII_lengths = range(13, 16) # 13, 14, 15\n",
        "\n",
        "    # Common metadata\n",
        "    variant_id = row.get(\"Uploaded_variation\")\n",
        "    gene = row.get(\"SYMBOL\")\n",
        "    transcript_id = row.get(\"Feature\")  # ENST\n",
        "    protein_id = row.get(\"translation_id\")\n",
        "    prot_pos = row.get(\"prot_pos\")\n",
        "    aa_wt = row.get(\"aa_wt\")\n",
        "    aa_alt = row.get(\"aa_alt\")\n",
        "\n",
        "    # ----- Class I: WT and MUT -----\n",
        "    ctx_I_wt = row.get(\"mhcI_wt_context\")\n",
        "    ctx_I_mut = row.get(\"mhcI_mut_context\")\n",
        "    idx_I_wt = mutated_index_in_context(row, FLANK_MHCI, \"mhcI_wt_context\")\n",
        "    idx_I_mut = mutated_index_in_context(row, FLANK_MHCI, \"mhcI_mut_context\")\n",
        "\n",
        "    for L in classI_lengths:\n",
        "        # WT\n",
        "        if idx_I_wt is not None and isinstance(ctx_I_wt, str):\n",
        "            for pep, mut_offset in windows_with_mutation(ctx_I_wt, idx_I_wt, L):\n",
        "                pair_id = f\"{variant_id}|I|{L}|{mut_offset}\"\n",
        "                records.append({\n",
        "                    \"variant_id\": variant_id,\n",
        "                    \"gene\": gene,\n",
        "                    \"transcript_id\": transcript_id,\n",
        "                    \"protein_id\": protein_id,\n",
        "                    \"class\": \"I\",\n",
        "                    \"type\": \"WT\",\n",
        "                    \"length\": L,\n",
        "                    \"peptide\": pep,\n",
        "                    \"mut_offset\": mut_offset,\n",
        "                    \"pair_id\": pair_id,\n",
        "                    \"prot_pos\": prot_pos,\n",
        "                    \"aa_wt\": aa_wt,\n",
        "                    \"aa_alt\": aa_alt,\n",
        "                })\n",
        "        # MUT\n",
        "        if idx_I_mut is not None and isinstance(ctx_I_mut, str):\n",
        "            for pep, mut_offset in windows_with_mutation(ctx_I_mut, idx_I_mut, L):\n",
        "                pair_id = f\"{variant_id}|I|{L}|{mut_offset}\"\n",
        "                records.append({\n",
        "                    \"variant_id\": variant_id,\n",
        "                    \"gene\": gene,\n",
        "                    \"transcript_id\": transcript_id,\n",
        "                    \"protein_id\": protein_id,\n",
        "                    \"class\": \"I\",\n",
        "                    \"type\": \"MUT\",\n",
        "                    \"length\": L,\n",
        "                    \"peptide\": pep,\n",
        "                    \"mut_offset\": mut_offset,\n",
        "                    \"pair_id\": pair_id,\n",
        "                    \"prot_pos\": prot_pos,\n",
        "                    \"aa_wt\": aa_wt,\n",
        "                    \"aa_alt\": aa_alt,\n",
        "                })\n",
        "\n",
        "    # ----- Class II: WT and MUT -----\n",
        "    ctx_II_wt = row.get(\"mhcII_wt_context\")\n",
        "    ctx_II_mut = row.get(\"mhcII_mut_context\")\n",
        "    idx_II_wt = mutated_index_in_context(row, FLANK_MHCII, \"mhcII_wt_context\")\n",
        "    idx_II_mut = mutated_index_in_context(row, FLANK_MHCII, \"mhcII_mut_context\")\n",
        "\n",
        "    for L in classII_lengths:\n",
        "        # WT\n",
        "        if idx_II_wt is not None and isinstance(ctx_II_wt, str):\n",
        "            for pep, mut_offset in windows_with_mutation(ctx_II_wt, idx_II_wt, L):\n",
        "                pair_id = f\"{variant_id}|II|{L}|{mut_offset}\"\n",
        "                records.append({\n",
        "                    \"variant_id\": variant_id,\n",
        "                    \"gene\": gene,\n",
        "                    \"transcript_id\": transcript_id,\n",
        "                    \"protein_id\": protein_id,\n",
        "                    \"class\": \"II\",\n",
        "                    \"type\": \"WT\",\n",
        "                    \"length\": L,\n",
        "                    \"peptide\": pep,\n",
        "                    \"mut_offset\": mut_offset,\n",
        "                    \"pair_id\": pair_id,\n",
        "                    \"prot_pos\": prot_pos,\n",
        "                    \"aa_wt\": aa_wt,\n",
        "                    \"aa_alt\": aa_alt,\n",
        "                })\n",
        "        # MUT\n",
        "        if idx_II_mut is not None and isinstance(ctx_II_mut, str):\n",
        "            for pep, mut_offset in windows_with_mutation(ctx_II_mut, idx_II_mut, L):\n",
        "                pair_id = f\"{variant_id}|II|{L}|{mut_offset}\"\n",
        "                records.append({\n",
        "                    \"variant_id\": variant_id,\n",
        "                    \"gene\": gene,\n",
        "                    \"transcript_id\": transcript_id,\n",
        "                    \"protein_id\": protein_id,\n",
        "                    \"class\": \"II\",\n",
        "                    \"type\": \"MUT\",\n",
        "                    \"length\": L,\n",
        "                    \"peptide\": pep,\n",
        "                    \"mut_offset\": mut_offset,\n",
        "                    \"pair_id\": pair_id,\n",
        "                    \"prot_pos\": prot_pos,\n",
        "                    \"aa_wt\": aa_wt,\n",
        "                    \"aa_alt\": aa_alt,\n",
        "                })\n",
        "\n",
        "    return records\n",
        "\n",
        "\n",
        "# Apply to all rows\n",
        "all_records = []\n",
        "for _, r in picked_df.iterrows():\n",
        "    all_records.extend(make_peptide_records_for_row(r))\n",
        "\n",
        "peptides_df = pd.DataFrame(all_records)\n",
        "print(\"Total peptides generated:\", peptides_df.shape[0])\n",
        "peptides_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------\n",
        "# Deduplicate and prepare NetMHCpan / NetMHCIIpan input files (PEPTIDE format)\n",
        "# --------------------------------------\n",
        "\n",
        "# Drop exact duplicate rows (same variant/gene/class/type/length/sequence)\n",
        "peptides_df = peptides_df.drop_duplicates(\n",
        "    subset=[\"variant_id\", \"gene\", \"class\", \"type\", \"length\", \"peptide\"]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Add a unique peptide_id for reference\n",
        "peptides_df[\"peptide_id\"] = peptides_df.index.to_series().map(lambda i: f\"pep{i:06d}\")\n",
        "\n",
        "print(\"Peptides after dropping duplicates:\", peptides_df.shape[0])\n",
        "\n",
        "# Split into Class I and Class II\n",
        "peptides_I = peptides_df[peptides_df[\"class\"] == \"I\"].copy()\n",
        "peptides_II = peptides_df[peptides_df[\"class\"] == \"II\"].copy()\n",
        "\n",
        "print(\"Class I peptides:\", peptides_I.shape[0])\n",
        "print(\"Class II peptides:\", peptides_II.shape[0])\n",
        "\n",
        "\n",
        "def write_plain_list(df, path):\n",
        "    \"\"\"\n",
        "    Write a simple list of peptides:\n",
        "        PEPTIDE1\n",
        "        PEPTIDE2\n",
        "        ...\n",
        "    This matches the simplest NetMHCpan / NetMHCIIpan format.\n",
        "    No empty lines are written.\n",
        "    \"\"\"\n",
        "    seqs = [str(p).strip().upper() for p in df[\"peptide\"]]\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(seqs) + \"\\n\")\n",
        "    print(f\"Wrote {len(seqs)} peptides to {path} (plain list)\")\n",
        "\n",
        "\n",
        "# --------- Output paths ---------\n",
        "NETMHCPAN_TXT   = \"netmhcpan_classI_peptides.txt\"\n",
        "NETMHCIIPAN_TXT = \"netmhciipan_classII_peptides.txt\"\n",
        "PEPTIDES_TSV    = \"peptides_for_netmhc.tsv\"\n",
        "\n",
        "# Write plain lists (PEPTIDE format)\n",
        "write_plain_list(peptides_I, NETMHCPAN_TXT)\n",
        "write_plain_list(peptides_II, NETMHCIIPAN_TXT)\n",
        "\n",
        "# Metadata table for later merging with NetMHC results\n",
        "# Includes: peptide_id, pair_id, mut_offset, variant_id, gene, class, type, length, peptide, etc.\n",
        "peptides_df.to_csv(PEPTIDES_TSV, sep=\"\\t\", index=False)\n",
        "print(f\"Saved peptide metadata table to {PEPTIDES_TSV}\")\n",
        "peptides_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
